<!DOCTYPE HTML>
<html lang="en">

<!-- https://stackoverflow.com/questions/13716658/how-to-delete-all-commit-history-in-github
how to delete old commits -->
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<meta name="description" content="LART Project Page">
	<meta name="author" content="Jathushan Rajasegaran">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<meta property="og:image" content="teaser.png">
	<meta property="og:type" content="website">
	<meta name="twitter:card" content="summary_large_image">
	<meta name="twitter:creator" content="@brjathu">

	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-H6N57KJW91"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());
		gtag('config', 'G-H6N57KJW91');
	</script>

  	<title>Jathushan Rajasegaran</title>
	
	<!-- redirect links from github.io to berkeley eecs site -->
	<!-- <script type="text/javascript">
		if (window.location == "https://brjathu.github.io/") {
		window.location.href = 'https://people.eecs.berkeley.edu/~jathushan/'; 
		}
	</script> -->

	<style>
		.image-container {
		  width: 200px;
		  height: 200px;
		  border-radius: 75%;
		  background-color: white;
		  overflow: hidden;
		}
		
		.image-container img {
		  width: 100%;
		  height: 100%;
		  object-fit: cover;
		}
	  </style>

	<meta name="author" content="Jathushan Rajasegaran">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	
	<link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" type="image/gif" href="images/icon.gif">

</head>


<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jathushan Rajasegaran</name>
              </p>
              <p>I am a Ph.D. student at <a href="https://bair.berkeley.edu/">BAIR</a> advised by Prof. <a href="http://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>. 

				I am broadly interested in Computer Vision and Deep Learning, with a focus on developing models for understanding long videos. 
				Before coming to Berkeley, I was working with Prof. <a href="https://salman-h-khan.github.io/">Salman Khan</a> at Inception Institute.
				I completed my undergraduate study at University of Moratuwa, with a major in Electronic and Telecommunication Engineering. 
				My Bachelor's Thesis was advised by Dr. <a href="https://ranga.staff.uom.lk/">Ranga Rodigo</a>. 

              </p>
              
              <p style="text-align:center">
                <!-- <a href="mailto:brjathu@gmail.com">Email</a> &nbsp/&nbsp -->
                <a href="mailto:jathushan@berkeley.edu">Email</a> &nbsp/&nbsp
                <!-- <a href="data/Jathushan-CV.pdf">CV</a> &nbsp/&nbsp -->
                <a href="bio.html">Bio</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=Ctp3igcAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                <a href="https://github.com/brjathu/">Github</a>&nbsp/&nbsp
				<!-- <a href="photos.html">Photos</a>&nbsp/&nbsp -->
				<a href="https://prettypixels.squarespace.com">Photos</a>&nbsp/&nbsp
				<a href="art.html">Art</a> <br>
				<!-- <a href="research.html">Research Statement (as of June 2023)</a>  -->
              </p>
			  <br>
			  <br>
		      <br>

			  <p>
				<i>Cars don't run like cheetahs, Planes don't fly like birds and Machines won't think in a way same as humans. They will do better. --Richard Feynman</i>
			  </p>
			  <!-- <p>
                <i>There are wavelengths that people cannot see, there are sounds that people cannot hear, and maybe computers have thoughts that people cannot think. --Richard W. Hamming</i>
			  </p> -->
			  <p>
				<i>We want AI agents that can discover like we can, not which contain what we have discovered. --Richard Sutton</i>
			  </p>
			  <p>
                <i>Machines should be able to understand the world outside our window. That world may change and evolve, 
				but the machines should perceive trees, cars and spaceships not pixels. --Max Wertheimer, Stan Lee</i>
			  </p> 
			  
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <!-- <a href="images/Jathushan.JPG"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Jathushan_circle.png" class="hoverZoomLink"></a> -->
              <!-- <a href="images/Jathushan2.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Jathushan2_circle.png" class="hoverZoomLink"></a> -->
			  <div class="image-container">
				<img src="images/Jathushan2.jpg" alt="Jathushan2">
			  </div>
            </td>
          </tr>
        </tbody></table>

		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
			<tr>
			<td style="padding:20px;width:100%;vertical-align:middle">
				<heading>Research</heading>
				<p>
				My research interests lie in the general area of computer vision and deep learning, 
				particularly in learning from videos, deep neural architectures and meta/continual learning. 
				<!-- Representative papers are <span class="highlight">highlighted</span>.                -->
				</p>
				<p style="text-align:left">
					<a href="research.html">Blog 1: Research Statement (as of June 2023)</a> 
				</p>
			</td>
			</tr>
		</tbody></table>
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				


<tr onmouseout="lart_animation_stop()" onmouseover="lart_animation_start()"  >
	<!-- <td style="padding:20px;width:25%;vertical-align:middle">
		<div class="one">
			</br></br>
			<div class="two" id='lart_image'>
				<video src='https://bfshi.github.io/images/humanoid_next_token_prediction.gif' width="160" type="video/mp4" id="teaser_video" autoplay loop></video>
			</div>
	</td> -->
	<td style="padding:20px;width:25%;vertical-align:middle">
		<div class="one">
			</br></br>
			<div class="two" id='www_image'>
				<img src="videos/humanoid_next_token_prediction.gif" width="160">
			</div>
		</div>
	</td>
	<td style="padding:20px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/abs/2402.19469">
			<papertitle>Humanoid Locomotion as Next Token Prediction</papertitle></a><br>	
			<a href="https://people.eecs.berkeley.edu/~ilija/">Ilija Radosavovic</a>,
			<a href="https://geopavlakos.github.io/">Bike Zhang</a>, 
			<a href="https://bfshi.github.io/">Baifeng Shi</a>,
			<strong>Jathushan Rajasegaran</strong>,			
			<a href="https://humanoid-next-token-prediction.github.io/">Sarthak Kamat</a>,<br>
			<a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
			<a href="https://hybrid-robotics.berkeley.edu/koushil/">Koushil Sreenath</a>, 
			<a href="http://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a> <br>
			<!-- <em>International Conference on Computer Vision (ICCV) </em>, 2023 &nbsp <br> -->
		<a href="https://humanoid-next-token-prediction.github.io/">project page</a>/
		<a href="https://arxiv.org/pdf/2402.19469">arxiv</a>/
		<!-- <a href="https://github.com/shubham-goel/4D-Humans">code</a>/ -->
		<!-- <a href="https://huggingface.co/spaces/brjathu/HMR2.0">demo</a>/ -->
		<p></p>
		<p>Real-world humanoid control as a next token prediction problem, akin to predicting the next word in language. </p>
	</td>
</tr>

<tr onmouseout="lart_animation_stop()" onmouseover="lart_animation_start()"  >
	<td style="padding:20px;width:25%;vertical-align:middle">
		<div class="one">
			</br></br>
			<div class="two" id='lart_image'>
				<video src='https://geopavlakos.github.io/files/hmr2.mp4' width="160" type="video/mp4" id="teaser_video" autoplay loop></video>
			</div>
	</td>
	<td style="padding:20px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/abs/2304.01199">
			<papertitle>Humans in 4D: Reconstructing and Tracking Humans with Transformers.</papertitle></a><br>	
			<a href="https://shubham-goel.github.io/">Shubham Goel</a>,
			<a href="https://geopavlakos.github.io/">Georgios Pavlakos</a>, 
			<strong>Jathushan Rajasegaran</strong>,				
			<a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a>, <br>
			<a href="http://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a> <br>
			<em>International Conference on Computer Vision (ICCV) </em>, 2023 &nbsp <br>
		<a href="https://shubham-goel.github.io/4dhumans/">project page</a>/
		<a href="https://arxiv.org/pdf/2305.20091.pdf">arxiv</a>/
		<a href="https://github.com/shubham-goel/4D-Humans">code</a>/
		<a href="https://huggingface.co/spaces/brjathu/HMR2.0">demo</a>/
		<p></p>
		<p>A fully "transformerized" deisgn for Human Mesh Recovery achieves improved precision and remarkable robustness for 3D human reconstruction and tracking!.</p>
	</td>
</tr>
			

<tr onmouseout="lart_animation_stop()" onmouseover="lart_animation_start()"  >
	<td style="padding:20px;width:25%;vertical-align:middle">
		<div class="one">
			</br></br>
			<div class="two" id='lart_image'>
				<video src='LART/teaser_page2.mp4' width="160" type="video/mp4" id="teaser_video" autoplay loop></video>
			</div>
	</td>
	<td style="padding:20px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/abs/2304.01199">
			<papertitle>On the Benefits of 3D Pose and Tracking for Human Action Recognition.</papertitle></a><br>	
		<strong>Jathushan Rajasegaran</strong>,				
		<a href="https://geopavlakos.github.io/">Georgios Pavlakos</a>, 
		<a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a>, <br>
		<a href="https://feichtenhofer.github.io/">Christoph Feichtenhofer</a>,
		<a href="http://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a> <br>
		<em>Computer Vision and Pattern Recognition (CVPR) </em>, 2023 &nbsp <br>
		<!-- <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rajasegaran_Tracking_People_by_Predicting_3D_Appearance_Location_and_Pose_CVPR_2022_paper.pdf">paper</a>/ -->
		<a href="LART/index.html">project page</a>/
		<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rajasegaran_On_the_Benefits_of_3D_Pose_and_Tracking_for_Human_CVPR_2023_paper.pdf">paper</a>/
		<a href="https://arxiv.org/abs/2304.01199">arxiv</a>/
		<a href="https://github.com/brjathu/LART">code</a>/
		<a href="https://colab.research.google.com/drive/1QRLqEAePmgS41v0KQwf87G_Ss_BLhPYs?usp=sharing">demo</a>/
		<a href="LART/files/submit_cvpr2023_poster.png">poster</a>
		<p></p>
		<p>Using 3D human reconstruction and tracking to recognize atomic actions in video.</p>
	</td>
</tr>


<tr onmouseout="phalp_animation_stop()" onmouseover="phalp_animation_start()"  >
	<td style="padding:20px;width:25%;vertical-align:middle">
		<div class="one">
			</br></br>
			<div class="two" id='phalp_image'>
				<img src='images/phalp_gif2.gif' width="160"></div>
			<img src='images/phalp_png.png' width="160"></div>
		<!-- <script type="text/javascript">function phalp_animation_start() {document.getElementById('phalp_image').style.opacity = "1";}
			function phalp_animation_stop() {document.getElementById('phalp_image').style.opacity = "0";}phalp_animation_stop()
		</script> -->
	</td>
	<td style="padding:20px;width:75%;vertical-align:middle">
		<a href="https://arxiv.org/pdf/2112.04477.pdf">
			<papertitle>Tracking People by Predicting 3D Appearance, Location and Pose.</papertitle></a><br>	
		<strong>Jathushan Rajasegaran</strong>,				
		<a href="https://geopavlakos.github.io/">Georgios Pavlakos</a>, 
		<a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a>,
		<a href="http://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>
		<br>
		<em>Computer Vision and Pattern Recognition (CVPR) </em>, 2022 <br>
		<font color="red"><strong>(Oral Presentation) <a href="https://youtu.be/GIOyBbSu2gw?t=3664"> <font color="red">(Best paper finalist - Top 0.4%)</font></a></strong></font>
		<br>
		<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rajasegaran_Tracking_People_by_Predicting_3D_Appearance_Location_and_Pose_CVPR_2022_paper.pdf">paper</a>/
		<a href="https://arxiv.org/pdf/2112.04477.pdf">arxiv</a>/
		<a href="PHALP/index.html">project page</a>/
		<a href="https://youtu.be/5xAbTpH8pO8">video</a>/
		<a href="https://www.youtube.com/watch?v=yY2hgeXn114">results</a>/
		<a href="PHALP/files/cvpr2022_poster.pdf">poster</a>/
		<a href="https://github.com/brjathu/PHALP">code</a>
		<p></p>
		<p>Performing monocular tracking of people by predicting their appearance, pose and location and in 3D.</p>
	</td>
</tr>


<tr onmouseout="t3po_animation_stop()" onmouseover="t3po_animation_start()"  >
	<!-- bgcolor="#ffffd0"   -->
	<td style="padding:20px;width:25%;vertical-align:middle">
		<div class="one">
			</br></br>
			<div class="two" id='t3po_image'>
				<img src='images/t3po_gif.gif' width="160"></div>
			<img src='images/t3po_png2.png' width="160"></div>
		<!-- <script type="text/javascript">function t3po_animation_start() {document.getElementById('t3po_image').style.opacity = "1";}
			function t3po_animation_stop() {document.getElementById('t3po_image').style.opacity = "0";}t3po_animation_stop()
		</script> -->
	</td>
	<td style="padding:20px;width:75%;vertical-align:middle">
		<a href="https://papers.nips.cc/paper/2021/hash/c74c4bf0dad9cbae3d80faa054b7d8ca-Abstract.html">
			<papertitle>Tracking People with 3D Representations.</papertitle></a><br>	
		<strong>Jathushan Rajasegaran</strong>,				
		<a href="https://geopavlakos.github.io/">Georgios Pavlakos</a>, 
		<a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a>,
		<a href="http://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>
		<br>
		<em>Neural Information Processing Systems (NeurIPS)</em>, 2021 
		<br>
		<a href="https://papers.nips.cc/paper/2021/hash/c74c4bf0dad9cbae3d80faa054b7d8ca-Abstract.html">paper</a>/
		<a href="https://arxiv.org/pdf/2111.07868.pdf">arxiv</a>/
		<a href="T3DP/index.html">project page</a>/
		<a href="https://www.youtube.com/watch?v=T4lCBfHWzC8">video</a>/
		<a href="https://github.com/brjathu/T3DP">code</a>/
		<a href="data/NeurIPS2021_poster.png">poster</a>
		<p></p>
		<p>Performing monocular tracking of people by lifting them to 3D and then using 3D representations of their appearance, pose and location.</p>
	</td>
</tr>




<tr onmouseout="itaml_animation_stop()" onmouseover="itaml_animation_start()"  >
	<!-- bgcolor="#ffffd0"  -->
	<td style="padding:20px;width:25%;vertical-align:middle">
		<div class="one">
			<div class="two" id='itaml_image'>
				<img src='images/itaml_gif.gif' width="160"></div>
			<img src='images/itaml_png.png' width="160"></div>
		<!-- <script type="text/javascript">function itaml_animation_start() {document.getElementById('itaml_image').style.opacity = "1";}
			function itaml_animation_stop() {document.getElementById('itaml_image').style.opacity = "0";}itaml_animation_stop()
		</script> -->
	</td>
	<td style="padding:20px;width:75%;vertical-align:middle">
		<a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Rajasegaran_iTAML_An_Incremental_Task-Agnostic_Meta-learning_Approach_CVPR_2020_paper.html">
			<papertitle>iTAML: An Incremental Task-Agnostic Meta-learning Approach.</papertitle></a><br>	
		<strong>Jathushan Rajasegaran</strong>,				
		<a href="https://salman-h-khan.github.io/">Salman Khan</a>, 
		<a href="https://mhayat.netlify.app/">Munawar Hayat</a>, 
		<a href="https://sites.google.com/view/fahadkhans/home">Fahad Shahbaz Khan</a>
		<br>
		<em>Computer Vision and Pattern Recognition (CVPR) </em>, 2020 
		<br>
		<a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Rajasegaran_iTAML_An_Incremental_Task-Agnostic_Meta-learning_Approach_CVPR_2020_paper.html">paper</a>/
		<a href="https://arxiv.org/abs/2003.11652">arxiv</a>/
		<a href="data/CVPR2020_slides.pdf">slides</a>/
		<a href="data/CVPR2020_1min.mp4">video</a>/
		<a href="https://github.com/brjathu/iTAML">code</a>
		<p></p>
		<p>By learning generic represenatations from past tasks, we can easily adapt to new tasks as well as remember old tasks.</p>
	</td>
</tr>





<tr onmouseout="rps_animation_stop()" onmouseover="rps_animation_start()" >
	<!-- bgcolor="#ffffd0"  -->
	<td style="padding:20px;width:25%;vertical-align:middle">
		<div class="one">
			<div class="two" id='rps_image'>
				<img src='images/rps_gif.gif' width="160"></div>
			<img src='images/rps_png.png' width="160"></div>
		<!-- <script type="text/javascript">function rps_animation_start() {document.getElementById('rps_image').style.opacity = "1";}
			function rps_animation_stop() {document.getElementById('rps_image').style.opacity = "0";}rps_animation_stop()
		</script> -->
	</td>
	<td style="padding:20px;width:75%;vertical-align:middle">
		<a href="https://papers.nips.cc/paper/2019/hash/83da7c539e1ab4e759623c38d8737e9e-Abstract.html">
			<papertitle>Random Path Selection for Incremental Learning.</papertitle></a><br>	
		<strong>Jathushan Rajasegaran</strong>,				
		<a href="https://mhayat.netlify.app/">Munawar Hayat</a>,
		<a href="https://salman-h-khan.github.io/">Salman Khan</a>,  <br>
		<a href="https://sites.google.com/view/fahadkhans/home">Fahad Shahbaz Khan</a>,
		<a href="http://www.inceptioniai.org/">Ling Shao</a>
		<br>
		<em>Neural Information Processing Systems (NeurIPS)</em>, 2019 
		<br>
		<a href="https://papers.nips.cc/paper/2019/hash/83da7c539e1ab4e759623c38d8737e9e-Abstract.html">paper</a>/
		<a href="https://arxiv.org/pdf/1906.01120.pdf">arxiv</a>/
		<a href="data/NeurIPS2019_Poster.pdf">poster</a>/
		<a href="https://github.com/brjathu/rps">code</a>
		<p></p>
		<p>We increase the width of a ResNet like model by adding extra skip connections when new tasks are introduced.</p>
	</td>
</tr>





<tr onmouseout="deepcaps_animation_stop()" onmouseover="deepcaps_animation_start()" >  
	<!-- bgcolor="#ffffd0"  -->
	<td style="padding:20px;width:25%;vertical-align:middle">
		<div class="one">
			<div class="two" id='deepcaps_image'>
				<img src='images/deepcaps_gif.gif' width="160"></div>
			<img src='images/deepcaps_png.png' height="160"></div>
		<!-- <script type="text/javascript">function deepcaps_animation_start() {document.getElementById('deepcaps_image').style.opacity = "1";}
			function deepcaps_animation_stop() {document.getElementById('deepcaps_image').style.opacity = "0";}deecaps_animation_stop()
		</script> -->
	</td>
	<td style="padding:20px;width:75%;vertical-align:middle">
		<a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Rajasegaran_DeepCaps_Going_Deeper_With_Capsule_Networks_CVPR_2019_paper.html">
			<papertitle>DeepCaps: Going Deeper with Capsule Networks</papertitle></a><br>
		<strong>Jathushan Rajasegaran</strong>,
		<a href="https://scholar.google.com/citations?user=2yTeZ58AAAAJ&hl=en">Vinoj Jayasundara</a>,
		<a href="https://scholar.google.com/citations?user=98ItndMAAAAJ&hl=en">Sandaru Jayasekara</a>, <br>
		<a href="https://scholar.google.com/citations?user=CFJHvLcAAAAJ&hl=en">Hirunima Jayasekara</a>,
		<a href="https://www.suranga.me/">Suranga Seneviratne</a>,
		<a href="https://ranga.staff.uom.lk/">Ranga Rodrigo</a>
		<br>
		<em>Computer Vision and Pattern Recognition (CVPR) </em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
		<br>
		<a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Rajasegaran_DeepCaps_Going_Deeper_With_Capsule_Networks_CVPR_2019_paper.html">paper</a>/
		<a href="data/CVPR19_Poster.pdf">poster</a>/
		<a href="https://www.youtube.com/watch?v=PzALQZOy09c&t=3600s">video</a>/
		<a href="https://github.com/brjathu/deepcaps">code</a>
		<p></p>
		<p>Capsule Networks are cool, but they are shallow. We can increase the depth by 3D convolutions and skip connections.</p>
	</td>
</tr>






<tr onmouseout="textcaps_animation_stop()" onmouseover="textcaps_animation_start()">  
	<td style="padding:20px;width:25%;vertical-align:middle">
		<div class="one">
			<div class="two" id='textcaps_image'>
				<img src='images/textcaps_gif.gif' width="160"></div>
			<img src='images/textcaps_png.png' height="160"></div>
		<!-- <script type="text/javascript">function textcaps_animation_start() {document.getElementById('textcaps_image').style.opacity = "1";}
			function textcaps_animation_stop() {document.getElementById('textcaps_image').style.opacity = "0";}textcaps_animation_stop()
		</script> -->
	</td>
	<td style="padding:20px;width:75%;vertical-align:middle">
		<a href="https://ieeexplore.ieee.org/abstract/document/8658735/">
			<papertitle>TextCaps: Handwritten Character Recognition with Very Small Datasets</papertitle></a><br>
		<a href="https://scholar.google.com/citations?user=2yTeZ58AAAAJ&hl=en">Vinoj Jayasundara</a>,
		<a href="https://scholar.google.com/citations?user=98ItndMAAAAJ&hl=en">Sandaru Jayasekara</a>,
		<a href="https://scholar.google.com/citations?user=CFJHvLcAAAAJ&hl=en">Hirunima Jayasekara</a>,
		<strong>Jathushan Rajasegaran</strong>, <br>
		<a href="https://www.suranga.me/">Suranga Seneviratne</a>,
		<a href="https://ranga.staff.uom.lk/">Ranga Rodrigo</a>
		<br>
		<em>Winter Conference on Applications of Computer Vision (WACV) </em>, 2019
		<br>
		<a href="https://ieeexplore.ieee.org/abstract/document/8658735/">paper</a>/
		<a href="https://arxiv.org/abs/1904.08095">arxiv</a>/
		<a href="data/WACV19_Poster.pdf">poster</a>/
		<a href="https://github.com/brjathu/textcaps">code</a>
		<p></p>
		<p>Capsule Networks can capture actual variations that are present in human hand writing, so we generate more data and retrain the capsule networks.</p>
	</td>
</tr>






<tr onmouseout="www_animation_stop()" onmouseover="www_animation_start()" >  
	<td style="padding:20px;width:25%;vertical-align:middle">
		<div class="one">
			<div class="two" id='www_image'>
				<img src='images/www_gif.gif' width="160"></div>
			<img src='images/www_png.png' height="160"></div>
		<!-- <script type="text/javascript">function www_animation_start() {document.getElementById('www_image').style.opacity = "1";}
			function www_animation_stop() {document.getElementById('www_image').style.opacity = "0";}www_animation_stop()
		</script> -->
	</td>
	<td style="padding:20px;width:75%;vertical-align:middle">
		<a href="https://dl.acm.org/doi/abs/10.1145/3308558.3313427">
			<papertitle>A Multi-modal Neural Embeddings Approach for Detecting Mobile Counterfeit Apps: A Case Study on Google Play Store</papertitle></a><br>
		<strong>Jathushan Rajasegaran</strong>, 
		<a href="https://naveenhk.github.io/">Naveen Karunanayake</a>,
		<a href="https://scholar.google.com.sg/citations?user=nELzVusAAAAJ&hl=en">Ashanie Gunathillake</a>,<br>
		<a href="https://scholar.google.com.sg/citations?user=eztAtiUAAAAJ&hl=en">Suranga Seneviratne</a>,
		<a href="https://www.suranga.me/">Guillaume Jourjon</a>
		<br>
		<em>International World Wide Web Conference (WWW)</em>, 2019
		<br>
		<a href="https://ieeexplore.ieee.org/abstract/document/9133509">paper</a>/
		<a href="https://arxiv.org/abs/1804.09882">arxiv</a>/
		<a href="data/WWW19_Poster.pdf">poster</a>/
		<p></p>
		<p>We use content and style representations detect counterfeit apps in playstore.</p>
	</td>
</tr>

		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
			<tbody>
				<tr>
				<td style="padding:0px">
					<br>
					<p style="text-align:right;font-size:small;">
					Website source from Jon Barron <a href="https://jonbarron.info/">here</a>
					<br>
					</p>
				</td>
				</tr>
		  	</tbody>
		</table>


      </td>
    </tr>
  </table>
</body>

</html>
