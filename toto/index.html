<html lang="en-GB">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TOTO</title>
    <meta name="description" content="Next token visual prediction models">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <meta name="robots" content="all">
    <meta content="en_EN" property="og:locale">
    <meta content="website" property="og:type">
    <meta content="https://shikun.io/projects/clarity" property="og:url">
    <meta content="TOTO" property="og:title">
    <meta content="Autoregressive Pre-training from Videos" property="og:description">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@your_twitter_id">
    <meta name="twitter:description" content="TOTO">
    <meta name="twitter:image:src" content="assets/figures/clarity.png">
    
    <link rel="stylesheet" type="text/css" media="all" href="assets/stylesheets/main_free.css" />
    <link rel="stylesheet" type="text/css" media="all" href="clarity/clarity.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/foundation.min.css">
    <link href="assets/fontawesome-free-6.6.0-web/css/all.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/styles.css"/>
    <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/index.js"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            "HTML-CSS": {
              scale: 95,
              fonts: ["Gyre-Pagella"],
              imageFont: null,
              undefinedFamily: "'Arial Unicode MS', cmbright"
            },
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                processEscapes: true
              }
          });
    </script>
    <script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-H6N57KJW91"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());
		gtag('config', 'G-BQQP1V77DH');
	</script>
</head>

<body>
    <!-- Title Page -->
    <!-- Dark Theme Example: Change the background colour dark and change the following div "blog-title" into "blog-title white". -->
    <div class="container blog" id="first-content" style="background-color: #043148;">
        <!-- If you don't have a project cover: Change "blog-title" into "blog-title no-cover"  -->
        <div class="blog-title white">
            <div class="blog-intro">
                <div>
                    <h1 class="title">An Empirical Study of Autoregressive Pre-training from Videos</h1>
                    <p class="author">
                        <a href="https://brjathu.github.io/">Jathushan Rajasegaran</a><sup>1,2</sup>, 
                        <a href="https://people.eecs.berkeley.edu/~ilija/">Ilija Radosavovic</a>,<sup>2</sup>, 
                        <a href="https://rravishankar1.github.io/">Rahul Ravishankar</a><sup>2</sup>, 
                        <a href="https://yossigandelsman.github.io/">Yossi Gandelsman</a><sup>1,2</sup>,
                        <a href="https://feichtenhofer.github.io/">Christoph Feichtenhofer</a><sup>1</sup>, 
                        <a href="http://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a><sup>1,2</sup>
                    </p>
                    
                    <p class="author" style="padding-top: 0px;">
                        <sup>1</sup> Meta AI, FAIR, <sup>2</sup> UC Berkeley
                    </p>

                    <p class="abstract">
                        We empirically study generative pre-training from videos. Our approach is conceptually simple and inspired by generative pre-training from text and images. To enable scaling to videos, we make several important improvements along the data, architecture, and evaluation axes. Our model, called <b>toto</b>, is a causal transformer that generates videos autoregressively, one token at a time. We pre-train our model on a diverse set of videos with over 1 trillion visual tokens. Our tokens are quantized patch embeddings,  
                        and we use relative embeddings for coarse-to-fine pre-training. We conduct a large-scale study across a suite of diverse benchmarks, including image recognition, video classification, object tracking, robotic manipulation and scaling behaviors. We find that, despite minimal inductive biases, our approach achieves competitive performance across all benchmarks. 
                    </p>

                  
                    <!-- Using FontAwesome Pro
                    <div class="info">
                        <div>
                            <a href="https://arxiv.org" class="button icon" style="background-color: rgba(255, 255, 255, 0.3)"> Paper <i class="far fa-book-open"></i></a> &nbsp;&nbsp; 
                            <a href="https://github.com" class="button icon" style="background-color: rgba(255, 255, 255, 0.3)">Code <i class="far fa-code"></i></a>  &nbsp;&nbsp; 
                            <a href="https://www.microsoft.com/en-gb/microsoft-365/powerpoint" class="button icon" style="background-color: rgba(255, 255, 255, 0.3);">Slides <i class="far fa-presentation"></i></a>  &nbsp;&nbsp; 
                            <a href="https://huggingface.co/spaces" class="button icon" style="background-color: rgba(255, 255, 255, 0.3)">Demo <i class="fa-light fa-face-smiling-hands"></i></a>
                        </div>
                    </div> -->

                    <!-- Using FontAwesome Free -->
                    <div class="info">
                        <div>
                            <a href="http://arxiv.org/abs/2501.05453" class="button icon" style="background-color: rgba(255, 255, 255, 0.2)"> Paper <i class="fa-solid fa-book-open"></i></a> &nbsp;&nbsp; 
                            <!-- <a href="https://github.com/brjathu/chewbacca" class="button icon" style="background-color: rgba(255, 255, 255, 0.2)">Code <i class="fa-solid fa-code"></i></a>  &nbsp;&nbsp;  -->
                            <!-- <a href="https://www.microsoft.com/en-gb/microsoft-365/powerpoint" class="button icon" style="background-color: rgba(255, 255, 255, 0.2);">Slides <i class="fa-regular fa-file-powerpoint"></i></a> &nbsp;&nbsp;  -->
                            <!-- <a href="https://huggingface.co/spaces/" class="button icon" style="background-color: rgba(255, 255, 255, 0.2)">Demo <i class="fa-solid fa-laptop-code"></i></a>  -->
                        </div>
                    </div>
                </div>
               
            </div>

            <div class="blog-cover">
                <img class="foreground" src="assets/figures/toto.png">
                <img class="background" src="assets/figures/toto.png">
            </div>
        </div>
    </div>


    <div class="container blog main first" id="blog-main">
        <h1 >
            Introduction
        </h1>
        <p class='text'>
        In a paper published in 1951, Shannon, having just published the foundational papers of information theory, proposed a “guessing game” of next word prediction to estimate the entropy of English. Nearly 70 years later, training a high-capacity transformer network on this task, provided the generative pre-training backbone for Large Language Models.
        </p>

        <p class='text'>
        Less well known is the fact that in 1954, Fred Attneave proposed an analog of Shannon’s task for images. To quote “We may divide the picture into arbitrarily small elements which we “transmit” to a subject (S) in a cumulative sequence, having them guess at the color of each successive element until they are correct. 
        This method of analysis resembles the scanning process used in television and facsimile systems and accomplishes the like purpose of transforming two spatial dimensions into a single sequence in time”. 
        </p>

        <!-- <p class='text'>
        While Attneave was concerned with images, in the context of 2024, we have to note the “Big Visual Data” is in videos. While there are concerns that most of the text available on the Internet has already been used by the language models, in video we just started on the journey of Big Data exploitation. 
        </p> -->

        <p class='text'>
            In this paper, we empirically study autoregressive pre-training from videos. To perform our empirical study, we construct a family of autoregressive video models which we call <b>Toto</b>. We treat videos as sequences of visual tokens and train a causal transformer models on next-token prediction task. 
We use causal transformer model with LLaMa architecture. We use  dVAE to tokenize frames into discrete tokens. Treating videos as sequences of tokens enables us to jointly train on videos and images using a unified format. We construct a diverse dataset of videos and images comprising over 1 trillion visual tokens. Our models are first pre-trained on this data and then evaluated on downstream tasks. We extract visual representations using attention pooling from relevant layers of the model.
        </p>

        <!-- <p class='text'>
        In summary, this paper studies two central questions: (1) what is an appropriate architecture for generative pre-training in vision, and (2) what are the benefits of generative pre-trained features for vision tasks? First, we study the effects of various tokenization approaches (e.g. VQGAN, dVAE, patches) and find that, most of these perform similar to each other. We find that relative positional embeddings are better than absolute ones, allowing us to extend the context length and resolution in a straightforward manner. Our architecture enables us to train models on videos and images jointly.
        </p> -->

        <!-- <p class='text'>
        We pre-train three model sizes, up to 1 billion parameters on a large set of videos and images containing over 1 trillion tokens or the equivalent of 144 thousand hours of videos. We evaluate these models on various tasks such as image recognition, action recognition, object tracking, object permanence, and object manipulation with robots. Our findings show that, with minimal inductive biases our autoregressive generative pre-trained models perform competitively to state-of-the-art approaches and show promising direction for training large-scale vision models on large quantities unfiltered video data. Finally, we study the scaling behaviours of <b>toto</b> and show a power law relationship of loss vs optimal compute. 
        </p> -->

    </div>

    <div class="container blog main blue", >
        <img src="assets/figures/toto_main.png">
        <p class="caption">
            <b>Overall Framework:</b> Starting with images and video frames from a collection of datasets, we tokenize each frame/image into discrete visual tokens independently.
    We pre-train the transformer by predicting the next visual tokens, with a context length of 4K tokens of images or video frames. Once trained, we take the intermediate representations and evaluate them on various tasks.    
        </p>
    </div>



    <div class="container blog main first" id="pretraining">
        <h1>Pre-training</h1>

            <p class="text">
                Given a large collection of images and videos, we tokenize all of them into a 1D sequence using raster scan ordering. This produces a dataset of tokens, $\{x^j_1, x^j_2, x^j_3, ..., x^j_n\}$ where $j$ is the sample either from a video or an image and $n$ is the number of tokens in an image or a video. We model the density $p(x)$ as:
            </p>
        
            <p class="text" style="text-align: center;">
                $$p(x^j) = \prod_{i=1}^{n} p(x^j_i | x^j_{i-1}, x^j_{i-2}, ..., x^j_{1}, \theta)$$
            </p>
        
            <p class="text">
                Here, $\theta$ is the model parameters, which can be optimized by minimizing the negative log-likelihood loss:
            </p>
        
            <p class="text" style="text-align: center;">
                $$\mathcal{L}_{\text{pre-train}} = \mathop{\mathbb{E}}_{x^j \sim X} -\log p(x^j).$$
            </p>
        
            <p class="text">
                Using this loss, we pre-train our models at different sizes on over one visual trillion tokens. These tokens are generated from images and video. The figure shows the training loss of 3 differently sized models with 120m, 280m and 1.1b parameters.
            </p>
    </div>

    <div class="container blog main blue", >
        <img src="assets/figures/toto_pretrain.png" style="width: 70%;">
        <p class="caption">
            <b>Training Loss Curves:</b> We show the training loss curves for base, large, and 1b models trained with tokens from dVAE with a vocabulary size of 8k and context length of 4k tokens (equivalent to 16 images or video frames).
        </p>
    </div>
    




    <div class="container blog main first" id="compute-scaling">
        <h1>Compute Optimal Scaling</h1>
       
            <p class="text">
                We study the scaling behaviours of toto using $\mu$-Parameterization. First we train various models a1-a6, with linearly increasing hidden size and number of layers, and we used VQGAN tokenizer. Then we tune the learning rate for these models, with $\mu$-Parameterization. The analysis shows optimal learning rate of $2^{-7}$ for all the model widths.
            </p>
        
            <p class="text">
                Once we find the optimal learning rate, we train a1-a6 models on our data mixture, as mentioned in the datasets table. The figure shows the loss vs compute of toto models. This shows a clear power law relationship with compute and validation loss. Based on these experiments toto shows a power law of:
            </p>
        
            <p class="text" style="text-align: center;">
                $$L(C) = 7.32 \cdot C^{-0.0378}$$
            </p>
        
            <p class="text">
                Interestingly, if we look at GPT3 power law relationship, it has:
            </p>
        
            <p class="text" style="text-align: center;">
                $$L(C) = 2.57 \cdot C^{-0.0480}$$
            </p>
        
            <p class="text">
                While these are not comparable directly, but the scaling coefficient shows how much change in loss for an added extra compute. This shows, that visual next token models such as toto scales, but at a slower rate than language only models.
            </p>
    </div>

    <div class="container blog main blue", >
        <img src="assets/figures/toto_scaling.png" style="width: 70%;">
        <p class="caption">
            <b>Training Loss Curves:</b> We show the training loss curves for base, large, and 1b models trained with tokens from dVAE with a vocabulary size of 8k and context length of 4k tokens (equivalent to 16 images or video frames).
        </p>
    </div>




    <div class="container blog main first" id="limitations">
        <h1>Experiments</h1>
        
        
        <div style="width: 80%; text-align: justify; margin: 0 auto 20px auto;">
            <div style="width: 50%; margin: 0 auto;">
                <img src="assets/figures/exp5.png" style="width: 100%;">
            </div>
            <p class="caption">
                <b>ImageNet Results:</b> We compare discriminative and generative models on ImageNet recognition task. While achieving comparable performance among generative models, our models model achieves the highest accuracy on autoregressive modeling. <sup>†</sup>models are evaluated with linear probing.
            </p>
        </div>

        <div style="width: 80%; text-align: justify; margin: 0 auto 20px auto;">
            <div style="width: 50%; margin: 0 auto;">
                <img src="assets/figures/exp6.png" style="width: 100%;">
            </div>
            <p class="caption">
                <b>K400 Results:</b> We compare discriminative and generative models on Kinetics-400 action recognition task. While achieving comparable performance among generative models, our models are the first to show the competitive performance on K400 with autoregressive pre-training, and shows scaling with large model sizes.
            </p>
        </div>

        <div style="width: 80%; text-align: justify; margin: 0 auto 20px auto;">
            <div style="width: 60%; margin: 0 auto;">
                <img src="assets/figures/exp7.png" style="width: 100%;">
            </div>
            <p class="caption">
                <b>Ego4D Results:</b> Our model achieves comparable mean-average precision compared to previous work. We compare our method with, FRCNN+Rnd, FRCNN+SF, Hiera, StillFast, VideoMAE, and MAE-ST.
            </p>
        </div>

        

        <!-- Figure with caption in the style of the template -->
        <div style="float: right; width: 100%; margin-left: 20px; margin-bottom: 20px;">
            <img src="assets/figures/exp1.png" style="width: 100%;">
            <p class="caption">
                <b>Probing Across Layers, Models, and Tasks:</b> We study the behavior of our models across multiple layers and tasks. For image classification, action recognition, and object tracking, all the models behave similarly and peak around 50% of the model depth. This behavior is observed across all model sizes. Robot tasks show a different behaviour, where the middle layers perform good at picking the object, last layers also perform good as middle layers.
            </p>
        </div>

        <div style="float: right; width: 100%; margin-left: 20px; margin-bottom: 20px;">
            <img src="assets/figures/exp2.png">
            <p class="caption">
                <b>Real-world Deployment:</b> We show an example episode of our policy performing the cube picking task on a Franka robot in the real world. We use toto-base to run the robot at real time, despite being a small model, toto was able to achieve about 63% success rate in real world setting.
            </p>
        </div>

        <div style="float: right; width: 100%; margin-left: 20px; margin-bottom: 20px;">
            <img src="assets/figures/exp3.png">
            <p class="caption">
                <b>Robot Manipulation Results:</b> We compare MAE-base with toto-base pre-trained models on robot manipulation. We evaluate each model the mean success rate over training steps. toto was able to learn these tasks faster than MAE, across two robots and two tasks.
            </p>
        </div>

        <div style="float: right; width: 100%; margin-left: 20px; margin-bottom: 20px;">
            <img src="assets/figures/exp4.png">
            <p class="caption">
                <b>Semi-Supervised Tracking:</b> We follow the protocol in STC, start with the GT segmentation mask, and propagate the labels using the features computed by toto-large. The mask was propagated up to 60 frames without losing much information.
            </p>
        </div>
    </div>
    


    





    <div class="container blog main first" id="limitations">
        <h1>Limitations</h1>
        
        <p class="text">
            In this work, we introduced toto, for generative pre-training from videos. Despite its competitive performance, this approach has limitations. A significant limitation stems from the use of internet videos, which, unlike carefully curated datasets, introduces challenges related to data quality and diversity. This variance in data quality can impact model performance, especially when compared to models trained on more curated datasets.
        </p>
    
        <p class="text">
            Another limitation is the use of tokenizer, this makes the learning not end-to-end, and the representation and generation quality is bounded by the quality of the tokenizer, and with quantized vectors, the quality is very much limited, this needs further explorations to build a universal visual tokenizer. Another fundamental limitation is training on videos for next token prediction task. The added redundancy in video frames, can hurt quality of the learned representations.
        </p>
    
        <p class="text">
            Additionally, our exploration of various design choices are based on ImageNet classification. While it does transfer to most of the tasks we considered in this paper, it may not be the optimal configuration for many other tasks.
        </p>
    
        <p class="text">
            Furthermore, we have not yet fully assessed our method's effectiveness in dealing with dense prediction tasks, fine-grained recognition, or comprehending complex temporal dynamics over extended time frames. These areas represent key opportunities for further research, aiming to broaden the fruitfulness of generative pre-trained models.
        </p>
    </div>



    <div class="container blog main first" id="conclusion">
        <h1>Conclusion</h1>
        
        <p class="text">
            We present an approach toto, for generative pre-training from videos. We build on prior work on generative pre-training from images and make architectural improvements to enable scaling to videos, including the use of quantized patch embeddings, relative position information. We curate a large video dataset and conduct a large-scale empirical study across a range of diverse tasks, including image recognition, video classification, object tracking, trajectory prediction, and robotic manipulation.
        </p>
    
        <p class="text">
            We perform extensive ablation studies to understand different design choices and compare our approach to strong baselines across different tasks. We find that, despite minimal inductive biases, our approach achieves competitive performance across all tasks. Finally, we studied the scaling behaviours of visual next token prediction models, and showed it scales with compute, but at a slower rate than text based next token prediction models.
        </p>
    </div>

    <div class="container blog main first" id="acknowledgments">
        <h1>Acknowledgments</h1>
        
        <p class="text">
            We thank Andrea Madotto, Po-Yao (Bernie) Huang, and Shiry Ginosar for helpful discussions. We're grateful to Ronghang Hu and Xinlei Chen for their help with TPU setup and code bases. We also thank Baifeng Shi for helping us with robots evaluations. We thank Valentin Gabeur and Neerja Thakkar for their valuable feedback on the paper.
        </p>
    </div>

    <div class="container blog main">
        <h1>Citation</h1>
        <pre><code class="plaintext">@article{autoregressive,
            title={Autoregressive Pre-training from Videos},
            author={Jathushan Rajasegaran, Ilija Radosavovic, Rahul Ravishankar, Yossi Gandelsman, Christoph Feichtenhofer,  Jitendra Malik},
            year={2024}
        }</code></pre>
    </div>

    <!-- Footer Page -->
    <footer>
        <div class="container">
            <p>
                This website is built on the <a href="https://shikun.io/projects/clarity">Clarity Template</a>, designed by <a href="https://shikun.io/">Shikun Liu</a>.
            </p>
        </div>    
    </footer>
    

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src="clarity/clarity.js"></script>    
    <script src="assets/scripts/main.js"></script>    
    </html>
</body>
